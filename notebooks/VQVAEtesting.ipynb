{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/scr/gmachi/prospector-guide/prospectors-v2/src')\n",
    "\n",
    "from data import retrieve_encoder, get_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, encoder_include):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.encoder_include = encoder_include\n",
    "\n",
    "        # Embedding table\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.encoder_include:\n",
    "            inputs = get_embeddings(self.encoder, inputs)\n",
    "            # NOTE: may need to implement batched version of above function\n",
    "        \n",
    "        # Flatten input for matching with embedding table\n",
    "        inputs_flat = inputs.view(-1, self.embedding_dim)\n",
    "\n",
    "        # Compute distances between input and embedding vectors\n",
    "        distances = torch.cdist(inputs_flat.unsqueeze(1), self.embeddings.weight.unsqueeze(0))\n",
    "\n",
    "        # Find nearest embedding index for each input\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)\n",
    "        quantized = self.embeddings(encoding_indices).view_as(inputs)\n",
    "\n",
    "        # Compute commitment loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # Pass-through gradient\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def forward(self, encoding_indices):\n",
    "        return self.embeddings(encoding_indices)\n",
    "\n",
    "class VQVAE1D(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25, encoder_include=False):\n",
    "        super(VQVAE1D, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encoder_include = encoder_include\n",
    "        if encoder_include:\n",
    "            self.encoder = retrieve_encoder()\n",
    "\n",
    "        # Vector quantizer\n",
    "        self.vector_quantizer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost, encoder_include)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Vector quantization\n",
    "        quantized, vq_loss, encoding_indices = self.vector_quantizer(inputs)\n",
    "\n",
    "        # Decode quantized representations\n",
    "        decoded = self.decoder(encoding_indices)\n",
    "\n",
    "        return decoded, vq_loss, encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 10, 1536])\n",
      "Decoded output shape: torch.Size([40, 1, 1536])\n",
      "VQ Loss: 1.247421383857727\n",
      "Encoding indices: tensor([[27],\n",
      "        [ 0],\n",
      "        [29],\n",
      "        [ 4],\n",
      "        [ 5],\n",
      "        [ 4],\n",
      "        [ 1],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [ 7],\n",
      "        [13],\n",
      "        [17],\n",
      "        [13],\n",
      "        [19],\n",
      "        [18],\n",
      "        [19],\n",
      "        [10],\n",
      "        [ 1],\n",
      "        [15],\n",
      "        [ 6],\n",
      "        [22],\n",
      "        [19],\n",
      "        [14],\n",
      "        [11],\n",
      "        [10],\n",
      "        [12],\n",
      "        [15],\n",
      "        [14],\n",
      "        [ 0],\n",
      "        [ 8],\n",
      "        [ 4],\n",
      "        [22],\n",
      "        [29],\n",
      "        [ 5],\n",
      "        [ 2],\n",
      "        [15],\n",
      "        [11],\n",
      "        [14],\n",
      "        [25],\n",
      "        [29]])\n"
     ]
    }
   ],
   "source": [
    "num_embeddings = 30 # this is K\n",
    "embedding_dim = 1536 # enter what we need for ESM3-open\n",
    "\n",
    "# Instantiate VQ-VAE\n",
    "vq_vae = VQVAE1D(num_embeddings, embedding_dim)\n",
    "\n",
    "# Example input (batch_size=4, sequence_length=10, embedding_dim=64)\n",
    "input_embeddings = torch.randn(4, 10, embedding_dim)\n",
    "print(\"Input shape:\", input_embeddings.shape)\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "decoded, vq_loss, encoding_indices = vq_vae(input_embeddings)\n",
    "\n",
    "print(\"Decoded output shape:\", decoded.shape)\n",
    "print(\"VQ Loss:\", vq_loss.item())\n",
    "print(\"Encoding indices:\", encoding_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Recon Loss: 0.9999, VQ Loss: 1.2477, Total Loss: 2.2476\n",
      "Epoch [20/100], Recon Loss: 0.9883, VQ Loss: 1.2333, Total Loss: 2.2215\n",
      "Epoch [30/100], Recon Loss: 0.9769, VQ Loss: 1.2191, Total Loss: 2.1960\n",
      "Epoch [40/100], Recon Loss: 0.9658, VQ Loss: 1.2053, Total Loss: 2.1711\n",
      "Epoch [50/100], Recon Loss: 0.9550, VQ Loss: 1.1917, Total Loss: 2.1467\n",
      "Epoch [60/100], Recon Loss: 0.9444, VQ Loss: 1.1785, Total Loss: 2.1230\n",
      "Epoch [70/100], Recon Loss: 0.9341, VQ Loss: 1.1656, Total Loss: 2.0997\n",
      "Epoch [80/100], Recon Loss: 0.9240, VQ Loss: 1.1530, Total Loss: 2.0770\n",
      "Epoch [90/100], Recon Loss: 0.9141, VQ Loss: 1.1407, Total Loss: 2.0548\n",
      "Epoch [100/100], Recon Loss: 0.9045, VQ Loss: 1.1287, Total Loss: 2.0331\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_embeddings = 30\n",
    "embedding_dim = 1536\n",
    "\n",
    "# Instantiate VQ-VAE\n",
    "vq_vae = VQVAE1D(num_embeddings, embedding_dim)\n",
    "optimizer = optim.Adam(vq_vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Example input (batch_size=4, sequence_length=10, embedding_dim=64)\n",
    "input_embeddings = torch.randn(4, 10, embedding_dim)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    vq_vae.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    decoded, vq_loss, _ = vq_vae(input_embeddings)\n",
    "\n",
    "    decoded = decoded.view_as(input_embeddings)\n",
    "    assert decoded.shape == input_embeddings.shape, f\"Shape mismatch: {decoded.shape} vs {input_embeddings.shape}\"\n",
    "\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = F.mse_loss(decoded, input_embeddings)\n",
    "    total_loss = recon_loss + vq_loss\n",
    "\n",
    "    # Backward pass\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print losses\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Recon Loss: {recon_loss.item():.4f}, VQ Loss: {vq_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "K = 5\n",
    "n = 3\n",
    "\n",
    "def create_kernel(n, K):\n",
    "    mono = list(range(K))\n",
    "    bi = [x for x in itertools.combinations(mono, 2)] + [(x, x) for x in mono]\n",
    "    kernel = dict.fromkeys(mono + bi, 0.0)\n",
    "    if n > 2:\n",
    "        tri = [x for x in itertools.combinations(mono, 3)] + [(x, x, x) for x in mono]\n",
    "        kernel = dict.fromkeys(mono + bi + tri, 0.0)  \n",
    "    return kernel\n",
    "\n",
    "len(create_kernel(n, K).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ph2esm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
